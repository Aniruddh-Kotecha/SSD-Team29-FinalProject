{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description: Next Code Token Prediction Using LSTM\n",
    "#### Project Overview:\n",
    "\n",
    "This project aims to develop a self-supervied learning model for predicting the next code token in a given code snippet. The model is built using Long Short-Term Memory (LSTM) networks, which are well-suited for sequence prediction tasks. The project includes the following steps:\n",
    "\n",
    "1- Data Collection: We use py150 from kaggle as our dataset. This rich, complex code provides a good challenge for our model.\n",
    "\n",
    "2- Data Preprocessing: The text data is tokenized, converted into sequences, and padded to ensure uniform input lengths. The sequences are then split into training and testing sets.\n",
    "\n",
    "3- Model Building: An LSTM model is constructed with an embedding layer, two LSTM layers, and a dense output layer with a softmax activation function to predict the probability of the next word.\n",
    "\n",
    "4- Model Training: The model is trained using the prepared sequences, with early stopping implemented to prevent overfitting. Early stopping monitors the validation loss and stops training when the loss stops improving.\n",
    "\n",
    "5- Model Evaluation: The model is evaluated using a set of example codes to test its ability to predict the next token accurately.\n",
    "\n",
    "6- Deployment: A Streamlit web application is developed to allow users to input a sequence of tokens and get the predicted next token in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235\n",
      "<class 'dict'>\n",
      "test_label.py ['from', '__future__', 'import', 'division', ',', 'print_function', ',', 'unicode_literals', 'import', 'sys', 'import', 'os', 'sys', '.', 'path', '.', 'insert', '(', '0', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', ',', \"'..'\", ')', ')', 'testinfo', '=', '\"s, t 5, s, t 10.1, s, q\"', 'tags', '=', '\"Label, text, ScaleTo\"', 'import', 'cocos', 'from', 'cocos', '.', 'director', 'import', 'director', 'from', 'cocos', '.', 'sprite', 'import', 'Sprite', 'from', 'cocos', '.', 'actions', 'import', '*', 'from', 'cocos', '.', 'text', 'import', '*', 'import', 'pyglet', 'class', 'TestLayer', '(', 'cocos', '.', 'layer', '.', 'Layer', ')', ':', '    ', 'def', '__init__', '(', 'self', ')', ':', '        ', 'super', '(', 'TestLayer', ',', 'self', ')', '.', '__init__', '(', ')', 'x', ',', 'y', '=', 'director', '.', 'get_window_size', '(', ')', 'self', '.', 'text', '=', 'Label', '(', '\"hello\"', ',', '(', 'x', '//', '2', ',', 'y', '//', '2', ')', ')', 'self', '.', 'text', '.', 'do', '(', 'Rotate', '(', '360', ',', '10', ')', ')', 'self', '.', 'text', '.', 'do', '(', 'ScaleTo', '(', '10', ',', '10', ')', ')', 'self', '.', 'add', '(', 'self', '.', 'text', ')', 'def', 'main', '(', ')', ':', '    ', 'director', '.', 'init', '(', ')', 'test_layer', '=', 'TestLayer', '(', ')', 'main_scene', '=', 'cocos', '.', 'scene', '.', 'Scene', '(', 'test_layer', ')', 'director', '.', 'run', '(', 'main_scene', ')', 'if', '__name__', '==', \"'__main__'\", ':', '    ', 'main', '(', ')']\n",
      "testcase.py ['import', 'unittest', 'class', 'BulbsTestCase', '(', 'unittest', '.', 'TestCase', ')', ':', '    ', 'client', '=', 'None', 'index_class', '=', 'None']\n",
      "test_dictfield.py ['from', '.', 'import', 'test_settings', 'from', 'datetime', 'import', 'date', 'from', 'rest_framework', '.', 'serializers', 'import', 'ValidationError', 'from', 'rest_framework', 'import', 'ISO_8601', 'from', 'rest_framework', '.', 'serializers', 'import', 'CharField', 'from', 'rest_framework', '.', 'serializers', 'import', 'DateField', 'import', 'pytest', 'from', 'drf_compound_fields', '.', 'fields', 'import', 'DictField', 'def', 'test_to_internal_value_with_child', '(', ')', ':', '    ', 'field', '=', 'DictField', '(', 'child', '=', 'DateField', '(', ')', ')', 'data', '=', '{', '\"a\"', ':', '\"2000-01-01\"', ',', '\"b\"', ':', '\"2000-01-02\"', '}', 'obj', '=', 'field', '.', 'to_internal_value', '(', 'data', ')', 'assert', '{', '\"a\"', ':', 'date', '(', '2000', ',', '1', ',', '1', ')', ',', '\"b\"', ':', 'date', '(', '2000', ',', '1', ',', '2', ')', '}', '==', 'obj', 'def', 'test_to_representation_with_child', '(', ')', ':', '    ', 'field', '=', 'DictField', '(', 'child', '=', 'DateField', '(', 'format', '=', 'ISO_8601', ')', ')', 'obj', '=', '{', '\"a\"', ':', 'date', '(', '2000', ',', '1', ',', '1', ')', ',', '\"b\"', ':', 'date', '(', '2000', ',', '1', ',', '2', ')', '}', 'data', '=', 'field', '.', 'to_representation', '(', 'obj', ')', 'assert', '{', '\"a\"', ':', '\"2000-01-01\"', ',', '\"b\"', ':', '\"2000-01-02\"', '}', '==', 'data', 'def', 'test_validate_non_dict', '(', ')', ':', '    ', 'field', '=', 'DictField', '(', 'child', '=', 'DateField', '(', ')', ')', 'with', 'pytest', '.', 'raises', '(', 'ValidationError', ')', ':', '        ', 'field', '.', 'to_internal_value', '(', \"'notADict'\", ')', 'def', 'test_validate_elements_valid', '(', ')', ':', '    ', 'field', '=', 'DictField', '(', 'child', '=', 'CharField', '(', 'max_length', '=', '5', ')', ')', 'try', ':', '        ', 'field', '.', 'to_internal_value', '(', '{', '\"a\"', ':', '\"a\"', ',', '\"b\"', ':', '\"b\"', ',', '\"c\"', ':', '\"c\"', '}', ')', 'except', 'ValidationError', ':', '        ', 'assert', 'False', ',', '\"ValidationError was raised\"', 'def', 'test_validate_elements_invalid', '(', ')', ':', '    ', 'field', '=', 'DictField', '(', 'child', '=', 'CharField', '(', 'max_length', '=', '5', ')', ')', 'with', 'pytest', '.', 'raises', '(', 'ValidationError', ')', ':', '        ', 'field', '.', 'to_internal_value', '(', '{', '\"a\"', ':', '\"012345\"', ',', '\"b\"', ':', '\"012345\"', '}', ')']\n",
      "test_create_instance.py ['from', 'awscli', '.', 'testutils', 'import', 'BaseAWSCommandParamsTest', 'import', 'awscli', '.', 'clidriver', 'class', 'TestCreateInstance', '(', 'BaseAWSCommandParamsTest', ')', ':', '    ', 'prefix', '=', \"'opsworks create-instance'\", 'def', 'test_simple', '(', 'self', ')', ':', '        ', 'cmdline', '=', 'self', '.', 'prefix', 'cmdline', '+=', \"' --stack-id f623987f-6303-4bba-a38e-63073e85c726'\", 'cmdline', '+=', \"' --layer-ids cb27894d-35f3-4435-b422-6641a785fa4a'\", 'cmdline', '+=', \"' --instance-type c1.medium'\", 'cmdline', '+=', \"' --hostname aws-client-instance'\", 'result', '=', '{', \"'StackId'\", ':', \"'f623987f-6303-4bba-a38e-63073e85c726'\", ',', \"'Hostname'\", ':', \"'aws-client-instance'\", ',', \"'LayerIds'\", ':', '[', \"'cb27894d-35f3-4435-b422-6641a785fa4a'\", ']', ',', \"'InstanceType'\", ':', \"'c1.medium'\", '}', 'self', '.', 'assert_params_for_cmd', '(', 'cmdline', ',', 'result', ')', 'if', '__name__', '==', '\"__main__\"', ':', '    ', 'unittest', '.', 'main', '(', ')']\n",
      "test_ajax.py ['from', 'bok_choy', '.', 'promise', 'import', 'BrokenPromise', 'from', 'bok_choy', '.', 'web_app_test', 'import', 'WebAppTest', 'from', '.', 'pages', 'import', 'AjaxPage', ',', 'AjaxNoJQueryPage', 'class', 'AjaxTest', '(', 'WebAppTest', ')', ':', '    ', 'def', 'setUp', '(', 'self', ')', ':', '        ', 'super', '(', 'AjaxTest', ',', 'self', ')', '.', 'setUp', '(', ')', 'self', '.', 'ajax', '=', 'AjaxPage', '(', 'self', '.', 'browser', ')', 'self', '.', 'ajax', '.', 'visit', '(', ')', 'def', 'test_ajax', '(', 'self', ')', ':', '        ', 'self', '.', 'ajax', '.', 'click_button', '(', ')', 'self', '.', 'ajax', '.', 'wait_for_ajax', '(', ')', 'self', '.', 'assertEquals', '(', 'self', '.', 'ajax', '.', 'output', ',', '\"Loaded via an ajax call.\"', ')', 'def', 'test_ajax_too_slow', '(', 'self', ')', ':', '        ', 'self', '.', 'ajax', '.', 'browser', '.', 'execute_script', '(', \"'jQuery.active=1'\", ')', 'with', 'self', '.', 'assertRaises', '(', 'BrokenPromise', ')', 'as', 'exc', ':', '            ', 'self', '.', 'ajax', '.', 'wait_for_ajax', '(', 'timeout', '=', '1', ')', 'self', '.', 'assertEqual', '(', \"'Promise not satisfied: Finished waiting for ajax requests.'\", ',', 'exc', '.', 'exception', '.', '__str__', '(', ')', ')', 'class', 'AjaxNoJQueryTest', '(', 'WebAppTest', ')', ':', '    ', 'def', 'setUp', '(', 'self', ')', ':', '        ', 'super', '(', 'AjaxNoJQueryTest', ',', 'self', ')', '.', 'setUp', '(', ')', 'self', '.', 'ajax', '=', 'AjaxNoJQueryPage', '(', 'self', '.', 'browser', ')', 'self', '.', 'ajax', '.', 'visit', '(', ')', 'def', 'test_ajax_with_slow_jquery', '(', 'self', ')', ':', '        ', 'with', 'self', '.', 'assertRaises', '(', 'BrokenPromise', ')', 'as', 'exc', ':', '            ', 'self', '.', 'ajax', '.', 'wait_for_ajax', '(', 'timeout', '=', '1', ')', 'self', '.', 'assertEqual', '(', \"'Promise not satisfied: Finished waiting for ajax requests.'\", ',', 'exc', '.', 'exception', '.', '__str__', '(', ')', ')']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tokenize\n",
    "\n",
    "\n",
    "def tokenize_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read().replace('\\t', '    ')  \n",
    "            tokens = tokenize.generate_tokens(iter(content.splitlines()).__next__)\n",
    "            token_list = []\n",
    "            for token in tokens:\n",
    "                if token.type not in (tokenize.COMMENT, tokenize.NL):\n",
    "                    if token.string == '':\n",
    "                        continue\n",
    "                    token_list.append(token.string)\n",
    "            return token_list\n",
    "    except tokenize.TokenError as e:\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def tokenize_directory(directory_path):\n",
    "    tokenized_files = {}\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.py'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            tokens = tokenize_file(file_path)\n",
    "            tokenized_files[filename] = tokens\n",
    "    \n",
    "    return tokenized_files\n",
    "\n",
    "\n",
    "directory_path = \"reduced_set\"\n",
    "\n",
    "tokenized_data = tokenize_directory(directory_path)\n",
    "\n",
    "print(len(tokenized_data))\n",
    "print(type(tokenized_data))\n",
    "\n",
    "for idx, (keys, values) in enumerate(tokenized_data.items()):\n",
    "    if idx >= 5:  \n",
    "        break\n",
    "    print(keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solver_id 1\n",
      "O_CREAT 2\n",
      "\"Method %s not found\" 3\n",
      "test_reduce 4\n",
      "'subfield2' 5\n",
      "test_label.py [20411, 24400, 11408, 28511, 9713, 7431, 9713, 42954, 11408, 23525, 11408, 16292, 23525, 10979, 29264, 10979, 30659, 10276, 26342, 9713, 16292, 10979, 29264, 10979, 37479, 10276, 16292, 10979, 29264, 10979, 4311, 10276, 6282, 2641, 9713, 7695, 2641, 2641, 29357, 38511, 13184, 26505, 38511, 41343, 11408, 13396, 20411, 13396, 10979, 34853, 11408, 34853, 20411, 13396, 10979, 9166, 11408, 26750, 20411, 13396, 10979, 15170, 11408, 40722, 20411, 13396, 10979, 21312, 11408, 40722, 11408, 26657, 34600, 8483, 10276, 13396, 10979, 10632, 10979, 5629, 2641, 32233, 40407, 23931, 34058, 10276, 41912, 2641, 32233, 37222, 23962, 10276, 8483, 9713, 41912, 2641, 10979, 34058, 10276, 2641, 6513, 9713, 42402, 38511, 34853, 10979, 34999, 10276, 2641, 41912, 10979, 21312, 38511, 7280, 10276, 41139, 9713, 10276, 6513, 5304, 19123, 9713, 42402, 5304, 19123, 2641, 2641, 41912, 10979, 21312, 10979, 33969, 10276, 7963, 10276, 616, 9713, 1032, 2641, 2641, 41912, 10979, 21312, 10979, 33969, 10276, 11594, 10276, 1032, 9713, 1032, 2641, 2641, 41912, 10979, 31404, 10276, 41912, 10979, 21312, 2641, 23931, 8109, 10276, 2641, 32233, 40407, 34853, 10979, 33800, 10276, 2641, 30310, 38511, 8483, 10276, 2641, 41365, 38511, 13396, 10979, 40085, 10979, 42478, 10276, 30310, 2641, 34853, 10979, 17906, 10276, 41365, 2641, 20269, 43281, 7323, 20876, 32233, 40407, 8109, 10276, 2641]\n",
      "testcase.py [11408, 36801, 34600, 39778, 10276, 36801, 10979, 28414, 2641, 32233, 40407, 40753, 38511, 25201, 34619, 38511, 25201]\n",
      "test_dictfield.py [20411, 10979, 11408, 27818, 20411, 5500, 11408, 6284, 20411, 942, 10979, 41195, 11408, 12795, 20411, 942, 11408, 6271, 20411, 942, 10979, 41195, 11408, 25321, 20411, 942, 10979, 41195, 11408, 35624, 11408, 10872, 20411, 17974, 10979, 9018, 11408, 28172, 23931, 34825, 10276, 2641, 32233, 40407, 7208, 38511, 28172, 10276, 10138, 38511, 35624, 10276, 2641, 2641, 23597, 38511, 22246, 16829, 32233, 39363, 9713, 5952, 32233, 34444, 30327, 25258, 38511, 7208, 10979, 660, 10276, 23597, 2641, 37427, 22246, 16829, 32233, 6284, 10276, 31270, 9713, 6373, 9713, 6373, 2641, 9713, 5952, 32233, 6284, 10276, 31270, 9713, 6373, 9713, 19123, 2641, 30327, 7323, 25258, 23931, 30973, 10276, 2641, 32233, 40407, 7208, 38511, 28172, 10276, 10138, 38511, 35624, 10276, 40931, 38511, 6271, 2641, 2641, 25258, 38511, 22246, 16829, 32233, 6284, 10276, 31270, 9713, 6373, 9713, 6373, 2641, 9713, 5952, 32233, 6284, 10276, 31270, 9713, 6373, 9713, 19123, 2641, 30327, 23597, 38511, 7208, 10979, 10292, 10276, 25258, 2641, 37427, 22246, 16829, 32233, 39363, 9713, 5952, 32233, 34444, 30327, 7323, 23597, 23931, 9045, 10276, 2641, 32233, 40407, 7208, 38511, 28172, 10276, 10138, 38511, 35624, 10276, 2641, 2641, 32957, 10872, 10979, 22773, 10276, 12795, 2641, 32233, 37222, 7208, 10979, 660, 10276, 31548, 2641, 23931, 1791, 10276, 2641, 32233, 40407, 7208, 38511, 28172, 10276, 10138, 38511, 25321, 10276, 10084, 38511, 22124, 2641, 2641, 300, 32233, 37222, 7208, 10979, 660, 10276, 22246, 16829, 32233, 16829, 9713, 5952, 32233, 5952, 9713, 10224, 32233, 10224, 30327, 2641, 38782, 12795, 32233, 37222, 37427, 25316, 9713, 15808, 23931, 33434, 10276, 2641, 32233, 40407, 7208, 38511, 28172, 10276, 10138, 38511, 25321, 10276, 10084, 38511, 22124, 2641, 2641, 32957, 10872, 10979, 22773, 10276, 12795, 2641, 32233, 37222, 7208, 10979, 660, 10276, 22246, 16829, 32233, 2532, 9713, 5952, 32233, 2532, 30327, 2641]\n",
      "test_create_instance.py [20411, 29029, 10979, 39009, 11408, 1643, 11408, 29029, 10979, 2703, 34600, 35036, 10276, 1643, 2641, 32233, 40407, 11849, 38511, 36236, 23931, 10231, 10276, 41912, 2641, 32233, 37222, 32149, 38511, 41912, 10979, 11849, 32149, 19738, 23692, 32149, 19738, 522, 32149, 19738, 27131, 32149, 19738, 32273, 1586, 38511, 22246, 42945, 32233, 3904, 9713, 11358, 32233, 31005, 9713, 3733, 32233, 2395, 9883, 39, 9713, 43815, 32233, 24856, 30327, 41912, 10979, 4010, 10276, 32149, 9713, 1586, 2641, 20269, 43281, 7323, 37597, 32233, 40407, 36801, 10979, 8109, 10276, 2641]\n",
      "test_ajax.py [20411, 24288, 10979, 25516, 11408, 28737, 20411, 24288, 10979, 2716, 11408, 42831, 20411, 10979, 31059, 11408, 34504, 9713, 40215, 34600, 19350, 10276, 42831, 2641, 32233, 40407, 23931, 13951, 10276, 41912, 2641, 32233, 37222, 23962, 10276, 19350, 9713, 41912, 2641, 10979, 13951, 10276, 2641, 41912, 10979, 39743, 38511, 34504, 10276, 41912, 10979, 37208, 2641, 41912, 10979, 39743, 10979, 20880, 10276, 2641, 23931, 22102, 10276, 41912, 2641, 32233, 37222, 41912, 10979, 39743, 10979, 11118, 10276, 2641, 41912, 10979, 39743, 10979, 39540, 10276, 2641, 41912, 10979, 13218, 10276, 41912, 10979, 39743, 10979, 1481, 9713, 43594, 2641, 23931, 19379, 10276, 41912, 2641, 32233, 37222, 41912, 10979, 39743, 10979, 37208, 10979, 14674, 10276, 36019, 2641, 32957, 41912, 10979, 18303, 10276, 28737, 2641, 1273, 29629, 32233, 11220, 41912, 10979, 39743, 10979, 39540, 10276, 15772, 38511, 6373, 2641, 41912, 10979, 27898, 10276, 12875, 9713, 29629, 10979, 13083, 10979, 34024, 10276, 2641, 2641, 34600, 17772, 10276, 42831, 2641, 32233, 40407, 23931, 13951, 10276, 41912, 2641, 32233, 37222, 23962, 10276, 17772, 9713, 41912, 2641, 10979, 13951, 10276, 2641, 41912, 10979, 39743, 38511, 40215, 10276, 41912, 10979, 37208, 2641, 41912, 10979, 39743, 10979, 20880, 10276, 2641, 23931, 8920, 10276, 41912, 2641, 32233, 37222, 32957, 41912, 10979, 18303, 10276, 28737, 2641, 1273, 29629, 32233, 11220, 41912, 10979, 39743, 10979, 39540, 10276, 15772, 38511, 6373, 2641, 41912, 10979, 27898, 10276, 12875, 9713, 29629, 10979, 13083, 10979, 34024, 10276, 2641, 2641]\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(tokenized_data):\n",
    "    \"\"\"\n",
    "    Build a vocabulary mapping tokens to unique IDs.\n",
    "    \"\"\"\n",
    "    all_tokens = [token for tokens in tokenized_data.values() for token in tokens]\n",
    "    vocab = {token: idx for idx, token in enumerate(set(all_tokens), start=1)}  \n",
    "    return vocab\n",
    "\n",
    "\n",
    "\n",
    "def convert_tokens_to_ids(tokenized_data, vocab):\n",
    "    \"\"\"\n",
    "    Convert tokenized data into token IDs using the vocabulary.\n",
    "    \"\"\"\n",
    "    token_ids_data = {\n",
    "        filename: [vocab[token] for token in tokens if token in vocab]\n",
    "        for filename, tokens in tokenized_data.items()\n",
    "    }\n",
    "    return token_ids_data\n",
    "\n",
    "vocab = build_vocabulary(tokenized_data)\n",
    "for idx, (keys, values) in enumerate(vocab.items()):\n",
    "    if idx >= 5:  \n",
    "        break\n",
    "    print(keys, values)\n",
    "\n",
    "token_ids_data = convert_tokens_to_ids(tokenized_data, vocab)\n",
    "\n",
    "for idx, (keys, values) in enumerate(token_ids_data.items()):\n",
    "    if idx >= 5:  \n",
    "        break\n",
    "    print(keys, values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"vocab1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(vocab, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "Input: [20411, 24400, 11408, 28511], Output: 9713\n",
      "Input: [24400, 11408, 28511, 9713], Output: 7431\n",
      "Input: [11408, 28511, 9713, 7431], Output: 9713\n",
      "Input: [28511, 9713, 7431, 9713], Output: 42954\n",
      "Input: [9713, 7431, 9713, 42954], Output: 11408\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequences(token_ids_data, sequence_length=4):\n",
    "    \"\"\"\n",
    "    Prepare input-output pairs for training a model.\n",
    "    Each input is a sequence of token IDs, and the output is the next token ID.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    for token_ids in token_ids_data.values():\n",
    "        for i in range(len(token_ids) - sequence_length):\n",
    "            input_seq = token_ids[i:i + sequence_length]\n",
    "            output_token = token_ids[i + sequence_length]\n",
    "            x.append(input_seq)\n",
    "            y.append(output_token)\n",
    "    return x, y\n",
    "\n",
    "sequence_length = 4\n",
    "x,y = prepare_sequences(token_ids_data, sequence_length)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "for i in range(5):  \n",
    "    print(f\"Input: {x[i]}, Output: {y[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814010 814010\n"
     ]
    }
   ],
   "source": [
    "print(len(x), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651208 651208 162802 162802\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(y_train), len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 00:38:01.365332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732907281.381006   72508 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732907281.385963   72508 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 00:38:01.403289: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/aniruddh/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "I0000 00:00:1732907282.956443   72508 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2791 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,GRU\n",
    "\n",
    "total_words = len(vocab) + 1\n",
    "max_sequence_len = 4\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words,activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732907285.986743   72594 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 16ms/step - accuracy: 0.2891 - loss: 4.8415 - val_accuracy: 0.4585 - val_loss: 3.7151\n",
      "Epoch 2/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 16ms/step - accuracy: 0.4794 - loss: 3.4349 - val_accuracy: 0.5032 - val_loss: 3.4204\n",
      "Epoch 3/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 16ms/step - accuracy: 0.5333 - loss: 2.9790 - val_accuracy: 0.5288 - val_loss: 3.2770\n",
      "Epoch 4/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 16ms/step - accuracy: 0.5675 - loss: 2.6814 - val_accuracy: 0.5452 - val_loss: 3.2065\n",
      "Epoch 5/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 16ms/step - accuracy: 0.5901 - loss: 2.4721 - val_accuracy: 0.5543 - val_loss: 3.1653\n",
      "Epoch 6/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 16ms/step - accuracy: 0.6069 - loss: 2.3125 - val_accuracy: 0.5581 - val_loss: 3.1314\n",
      "Epoch 7/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.6211 - loss: 2.1862 - val_accuracy: 0.5650 - val_loss: 3.1230\n",
      "Epoch 8/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.6321 - loss: 2.0787 - val_accuracy: 0.5669 - val_loss: 3.1263\n",
      "Epoch 9/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.6418 - loss: 1.9902 - val_accuracy: 0.5682 - val_loss: 3.1346\n",
      "Epoch 10/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.6501 - loss: 1.9103 - val_accuracy: 0.5685 - val_loss: 3.1688\n",
      "Epoch 11/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.6550 - loss: 1.8524 - val_accuracy: 0.5725 - val_loss: 3.1672\n",
      "Epoch 12/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 16ms/step - accuracy: 0.6644 - loss: 1.7813 - val_accuracy: 0.5741 - val_loss: 3.1847\n",
      "Epoch 13/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.6687 - loss: 1.7324 - val_accuracy: 0.5727 - val_loss: 3.2078\n",
      "Epoch 14/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 16ms/step - accuracy: 0.6732 - loss: 1.6838 - val_accuracy: 0.5731 - val_loss: 3.2404\n",
      "Epoch 15/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.6779 - loss: 1.6400 - val_accuracy: 0.5751 - val_loss: 3.2616\n",
      "Epoch 16/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.6830 - loss: 1.5992 - val_accuracy: 0.5768 - val_loss: 3.2740\n",
      "Epoch 17/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.6871 - loss: 1.5652 - val_accuracy: 0.5761 - val_loss: 3.3095\n",
      "Epoch 18/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.6914 - loss: 1.5298 - val_accuracy: 0.5780 - val_loss: 3.3155\n",
      "Epoch 19/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.6960 - loss: 1.4959 - val_accuracy: 0.5753 - val_loss: 3.3703\n",
      "Epoch 20/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.6985 - loss: 1.4675 - val_accuracy: 0.5778 - val_loss: 3.3595\n",
      "Epoch 21/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.7022 - loss: 1.4405 - val_accuracy: 0.5760 - val_loss: 3.4141\n",
      "Epoch 22/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.7061 - loss: 1.4140 - val_accuracy: 0.5777 - val_loss: 3.4107\n",
      "Epoch 23/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.7084 - loss: 1.3946 - val_accuracy: 0.5794 - val_loss: 3.4389\n",
      "Epoch 24/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.7110 - loss: 1.3746 - val_accuracy: 0.5797 - val_loss: 3.4427\n",
      "Epoch 25/25\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7138 - loss: 1.3530"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=25,validation_data=(x_test,y_test),verbose=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]  \n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"next_word_lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
