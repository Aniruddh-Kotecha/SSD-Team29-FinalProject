{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description: Next Word Prediction Using LSTM\n",
    "#### Project Overview:\n",
    "\n",
    "This project aims to develop a deep learning model for predicting the next word in a given sequence of words. The model is built using Long Short-Term Memory (LSTM) networks, which are well-suited for sequence prediction tasks. The project includes the following steps:\n",
    "\n",
    "1- Data Collection: We use the text of Shakespeare's \"Hamlet\" as our dataset. This rich, complex text provides a good challenge for our model.\n",
    "\n",
    "2- Data Preprocessing: The text data is tokenized, converted into sequences, and padded to ensure uniform input lengths. The sequences are then split into training and testing sets.\n",
    "\n",
    "3- Model Building: An LSTM model is constructed with an embedding layer, two LSTM layers, and a dense output layer with a softmax activation function to predict the probability of the next word.\n",
    "\n",
    "4- Model Training: The model is trained using the prepared sequences, with early stopping implemented to prevent overfitting. Early stopping monitors the validation loss and stops training when the loss stops improving.\n",
    "\n",
    "5- Model Evaluation: The model is evaluated using a set of example sentences to test its ability to predict the next word accurately.\n",
    "\n",
    "6- Deployment: A Streamlit web application is developed to allow users to input a sequence of words and get the predicted next word in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/aniruddh/anaconda3/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/aniruddh/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235\n",
      "<class 'dict'>\n",
      "test_label.py ['from', '__future__', 'import', 'division', ',', 'print_function', ',', 'unicode_literals', 'import', 'sys', 'import', 'os', 'sys', '.', 'path', '.', 'insert', '(', '0', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', ',', \"'..'\", ')', ')', 'testinfo', '=', '\"s, t 5, s, t 10.1, s, q\"', 'tags', '=', '\"Label, text, ScaleTo\"', 'import', 'cocos', 'from', 'cocos', '.', 'director', 'import', 'director', 'from', 'cocos', '.', 'sprite', 'import', 'Sprite', 'from', 'cocos', '.', 'actions', 'import', '*', 'from', 'cocos', '.', 'text', 'import', '*', 'import', 'pyglet', 'class', 'TestLayer', '(', 'cocos', '.', 'layer', '.', 'Layer', ')', ':', '    ', 'def', '__init__', '(', 'self', ')', ':', '        ', 'super', '(', 'TestLayer', ',', 'self', ')', '.', '__init__', '(', ')', 'x', ',', 'y', '=', 'director', '.', 'get_window_size', '(', ')', 'self', '.', 'text', '=', 'Label', '(', '\"hello\"', ',', '(', 'x', '//', '2', ',', 'y', '//', '2', ')', ')', 'self', '.', 'text', '.', 'do', '(', 'Rotate', '(', '360', ',', '10', ')', ')', 'self', '.', 'text', '.', 'do', '(', 'ScaleTo', '(', '10', ',', '10', ')', ')', 'self', '.', 'add', '(', 'self', '.', 'text', ')', 'def', 'main', '(', ')', ':', '    ', 'director', '.', 'init', '(', ')', 'test_layer', '=', 'TestLayer', '(', ')', 'main_scene', '=', 'cocos', '.', 'scene', '.', 'Scene', '(', 'test_layer', ')', 'director', '.', 'run', '(', 'main_scene', ')', 'if', '__name__', '==', \"'__main__'\", ':', '    ', 'main', '(', ')']\n",
      "testcase.py ['import', 'unittest', 'class', 'BulbsTestCase', '(', 'unittest', '.', 'TestCase', ')', ':', '    ', 'client', '=', 'None', 'index_class', '=', 'None']\n",
      "test_dictfield.py ['from', '.', 'import', 'test_settings', 'from', 'datetime', 'import', 'date', 'from', 'rest_framework', '.', 'serializers', 'import', 'ValidationError', 'from', 'rest_framework', 'import', 'ISO_8601', 'from', 'rest_framework', '.', 'serializers', 'import', 'CharField', 'from', 'rest_framework', '.', 'serializers', 'import', 'DateField', 'import', 'pytest', 'from', 'drf_compound_fields', '.', 'fields', 'import', 'DictField', 'def', 'test_to_internal_value_with_child', '(', ')', ':', '    ', 'field', '=', 'DictField', '(', 'child', '=', 'DateField', '(', ')', ')', 'data', '=', '{', '\"a\"', ':', '\"2000-01-01\"', ',', '\"b\"', ':', '\"2000-01-02\"', '}', 'obj', '=', 'field', '.', 'to_internal_value', '(', 'data', ')', 'assert', '{', '\"a\"', ':', 'date', '(', '2000', ',', '1', ',', '1', ')', ',', '\"b\"', ':', 'date', '(', '2000', ',', '1', ',', '2', ')', '}', '==', 'obj', 'def', 'test_to_representation_with_child', '(', ')', ':', '    ', 'field', '=', 'DictField', '(', 'child', '=', 'DateField', '(', 'format', '=', 'ISO_8601', ')', ')', 'obj', '=', '{', '\"a\"', ':', 'date', '(', '2000', ',', '1', ',', '1', ')', ',', '\"b\"', ':', 'date', '(', '2000', ',', '1', ',', '2', ')', '}', 'data', '=', 'field', '.', 'to_representation', '(', 'obj', ')', 'assert', '{', '\"a\"', ':', '\"2000-01-01\"', ',', '\"b\"', ':', '\"2000-01-02\"', '}', '==', 'data', 'def', 'test_validate_non_dict', '(', ')', ':', '    ', 'field', '=', 'DictField', '(', 'child', '=', 'DateField', '(', ')', ')', 'with', 'pytest', '.', 'raises', '(', 'ValidationError', ')', ':', '        ', 'field', '.', 'to_internal_value', '(', \"'notADict'\", ')', 'def', 'test_validate_elements_valid', '(', ')', ':', '    ', 'field', '=', 'DictField', '(', 'child', '=', 'CharField', '(', 'max_length', '=', '5', ')', ')', 'try', ':', '        ', 'field', '.', 'to_internal_value', '(', '{', '\"a\"', ':', '\"a\"', ',', '\"b\"', ':', '\"b\"', ',', '\"c\"', ':', '\"c\"', '}', ')', 'except', 'ValidationError', ':', '        ', 'assert', 'False', ',', '\"ValidationError was raised\"', 'def', 'test_validate_elements_invalid', '(', ')', ':', '    ', 'field', '=', 'DictField', '(', 'child', '=', 'CharField', '(', 'max_length', '=', '5', ')', ')', 'with', 'pytest', '.', 'raises', '(', 'ValidationError', ')', ':', '        ', 'field', '.', 'to_internal_value', '(', '{', '\"a\"', ':', '\"012345\"', ',', '\"b\"', ':', '\"012345\"', '}', ')']\n",
      "test_create_instance.py ['from', 'awscli', '.', 'testutils', 'import', 'BaseAWSCommandParamsTest', 'import', 'awscli', '.', 'clidriver', 'class', 'TestCreateInstance', '(', 'BaseAWSCommandParamsTest', ')', ':', '    ', 'prefix', '=', \"'opsworks create-instance'\", 'def', 'test_simple', '(', 'self', ')', ':', '        ', 'cmdline', '=', 'self', '.', 'prefix', 'cmdline', '+=', \"' --stack-id f623987f-6303-4bba-a38e-63073e85c726'\", 'cmdline', '+=', \"' --layer-ids cb27894d-35f3-4435-b422-6641a785fa4a'\", 'cmdline', '+=', \"' --instance-type c1.medium'\", 'cmdline', '+=', \"' --hostname aws-client-instance'\", 'result', '=', '{', \"'StackId'\", ':', \"'f623987f-6303-4bba-a38e-63073e85c726'\", ',', \"'Hostname'\", ':', \"'aws-client-instance'\", ',', \"'LayerIds'\", ':', '[', \"'cb27894d-35f3-4435-b422-6641a785fa4a'\", ']', ',', \"'InstanceType'\", ':', \"'c1.medium'\", '}', 'self', '.', 'assert_params_for_cmd', '(', 'cmdline', ',', 'result', ')', 'if', '__name__', '==', '\"__main__\"', ':', '    ', 'unittest', '.', 'main', '(', ')']\n",
      "test_ajax.py ['from', 'bok_choy', '.', 'promise', 'import', 'BrokenPromise', 'from', 'bok_choy', '.', 'web_app_test', 'import', 'WebAppTest', 'from', '.', 'pages', 'import', 'AjaxPage', ',', 'AjaxNoJQueryPage', 'class', 'AjaxTest', '(', 'WebAppTest', ')', ':', '    ', 'def', 'setUp', '(', 'self', ')', ':', '        ', 'super', '(', 'AjaxTest', ',', 'self', ')', '.', 'setUp', '(', ')', 'self', '.', 'ajax', '=', 'AjaxPage', '(', 'self', '.', 'browser', ')', 'self', '.', 'ajax', '.', 'visit', '(', ')', 'def', 'test_ajax', '(', 'self', ')', ':', '        ', 'self', '.', 'ajax', '.', 'click_button', '(', ')', 'self', '.', 'ajax', '.', 'wait_for_ajax', '(', ')', 'self', '.', 'assertEquals', '(', 'self', '.', 'ajax', '.', 'output', ',', '\"Loaded via an ajax call.\"', ')', 'def', 'test_ajax_too_slow', '(', 'self', ')', ':', '        ', 'self', '.', 'ajax', '.', 'browser', '.', 'execute_script', '(', \"'jQuery.active=1'\", ')', 'with', 'self', '.', 'assertRaises', '(', 'BrokenPromise', ')', 'as', 'exc', ':', '            ', 'self', '.', 'ajax', '.', 'wait_for_ajax', '(', 'timeout', '=', '1', ')', 'self', '.', 'assertEqual', '(', \"'Promise not satisfied: Finished waiting for ajax requests.'\", ',', 'exc', '.', 'exception', '.', '__str__', '(', ')', ')', 'class', 'AjaxNoJQueryTest', '(', 'WebAppTest', ')', ':', '    ', 'def', 'setUp', '(', 'self', ')', ':', '        ', 'super', '(', 'AjaxNoJQueryTest', ',', 'self', ')', '.', 'setUp', '(', ')', 'self', '.', 'ajax', '=', 'AjaxNoJQueryPage', '(', 'self', '.', 'browser', ')', 'self', '.', 'ajax', '.', 'visit', '(', ')', 'def', 'test_ajax_with_slow_jquery', '(', 'self', ')', ':', '        ', 'with', 'self', '.', 'assertRaises', '(', 'BrokenPromise', ')', 'as', 'exc', ':', '            ', 'self', '.', 'ajax', '.', 'wait_for_ajax', '(', 'timeout', '=', '1', ')', 'self', '.', 'assertEqual', '(', \"'Promise not satisfied: Finished waiting for ajax requests.'\", ',', 'exc', '.', 'exception', '.', '__str__', '(', ')', ')']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tokenize\n",
    "\n",
    "\n",
    "def tokenize_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "            content = file.read().replace('\\t', '    ')  \n",
    "            tokens = tokenize.generate_tokens(iter(content.splitlines()).__next__)\n",
    "            token_list = []\n",
    "            for token in tokens:\n",
    "                if token.type not in (tokenize.COMMENT, tokenize.NL):\n",
    "                    if token.string == '':\n",
    "                        continue\n",
    "                    token_list.append(token.string)\n",
    "\n",
    "            return token_list\n",
    "    except tokenize.TokenError as e:\n",
    "\n",
    "        return []\n",
    "    except Exception as e:\n",
    "\n",
    "        return []\n",
    "\n",
    "def tokenize_directory(directory_path):\n",
    "    tokenized_files = {}\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.py'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            tokens = tokenize_file(file_path)\n",
    "            tokenized_files[filename] = tokens\n",
    "    \n",
    "    return tokenized_files\n",
    "\n",
    "\n",
    "\n",
    "directory_path = \"reduced_set\"\n",
    "\n",
    "tokenized_data = tokenize_directory(directory_path)\n",
    "\n",
    "print(len(tokenized_data))\n",
    "print(type(tokenized_data))\n",
    "\n",
    "for idx, (keys, values) in enumerate(tokenized_data.items()):\n",
    "    if idx >= 5:  \n",
    "        break\n",
    "    print(keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_sql 1\n",
      "'first' 2\n",
      "'project' 3\n",
      "value_to_name 4\n",
      "'aaaa-bb-cc' 5\n",
      "test_label.py [34558, 34404, 18017, 13959, 33166, 25037, 33166, 7651, 18017, 13197, 18017, 7844, 13197, 15467, 67, 15467, 616, 34818, 13840, 33166, 7844, 15467, 67, 15467, 17795, 34818, 7844, 15467, 67, 15467, 24403, 34818, 42033, 18470, 33166, 23120, 18470, 18470, 7689, 19052, 41055, 37346, 19052, 5004, 18017, 12209, 34558, 12209, 15467, 12325, 18017, 12325, 34558, 12209, 15467, 8490, 18017, 20911, 34558, 12209, 15467, 18602, 18017, 32224, 34558, 12209, 15467, 39537, 18017, 32224, 18017, 6769, 903, 41756, 34818, 12209, 15467, 43835, 15467, 42572, 18470, 2006, 21263, 9014, 39944, 34818, 8491, 18470, 2006, 33512, 37221, 34818, 41756, 33166, 8491, 18470, 15467, 39944, 34818, 18470, 7757, 33166, 7600, 19052, 12325, 15467, 25594, 34818, 18470, 8491, 15467, 39537, 19052, 15124, 34818, 3832, 33166, 34818, 7757, 28796, 36508, 33166, 7600, 28796, 36508, 18470, 18470, 8491, 15467, 39537, 15467, 37464, 34818, 20537, 34818, 28271, 33166, 41351, 18470, 18470, 8491, 15467, 39537, 15467, 37464, 34818, 14759, 34818, 41351, 33166, 41351, 18470, 18470, 8491, 15467, 21604, 34818, 8491, 15467, 39537, 18470, 9014, 4884, 34818, 18470, 2006, 21263, 12325, 15467, 41298, 34818, 18470, 25605, 19052, 41756, 34818, 18470, 26456, 19052, 12209, 15467, 25290, 15467, 38187, 34818, 25605, 18470, 12325, 15467, 39264, 34818, 26456, 18470, 2925, 1379, 1478, 38424, 2006, 21263, 4884, 34818, 18470]\n",
      "testcase.py [18017, 23280, 903, 22748, 34818, 23280, 15467, 28539, 18470, 2006, 21263, 32636, 19052, 21554, 27620, 19052, 21554]\n",
      "test_dictfield.py [34558, 15467, 18017, 25869, 34558, 33646, 18017, 24653, 34558, 17171, 15467, 38188, 18017, 41158, 34558, 17171, 18017, 13194, 34558, 17171, 15467, 38188, 18017, 30200, 34558, 17171, 15467, 38188, 18017, 36496, 18017, 7575, 34558, 23164, 15467, 30775, 18017, 20772, 9014, 38714, 34818, 18470, 2006, 21263, 10027, 19052, 20772, 34818, 9390, 19052, 36496, 34818, 18470, 18470, 21892, 19052, 18154, 24240, 2006, 4023, 33166, 6863, 2006, 43888, 29781, 24687, 19052, 10027, 15467, 37723, 34818, 21892, 18470, 43956, 18154, 24240, 2006, 24653, 34818, 18984, 33166, 13612, 33166, 13612, 18470, 33166, 6863, 2006, 24653, 34818, 18984, 33166, 13612, 33166, 36508, 18470, 29781, 1478, 24687, 9014, 39866, 34818, 18470, 2006, 21263, 10027, 19052, 20772, 34818, 9390, 19052, 36496, 34818, 24577, 19052, 13194, 18470, 18470, 24687, 19052, 18154, 24240, 2006, 24653, 34818, 18984, 33166, 13612, 33166, 13612, 18470, 33166, 6863, 2006, 24653, 34818, 18984, 33166, 13612, 33166, 36508, 18470, 29781, 21892, 19052, 10027, 15467, 39617, 34818, 24687, 18470, 43956, 18154, 24240, 2006, 4023, 33166, 6863, 2006, 43888, 29781, 1478, 21892, 9014, 1327, 34818, 18470, 2006, 21263, 10027, 19052, 20772, 34818, 9390, 19052, 36496, 34818, 18470, 18470, 14168, 7575, 15467, 28312, 34818, 41158, 18470, 2006, 33512, 10027, 15467, 37723, 34818, 24331, 18470, 9014, 7663, 34818, 18470, 2006, 21263, 10027, 19052, 20772, 34818, 9390, 19052, 30200, 34818, 5972, 19052, 33653, 18470, 18470, 20143, 2006, 33512, 10027, 15467, 37723, 34818, 18154, 24240, 2006, 24240, 33166, 6863, 2006, 6863, 33166, 32034, 2006, 32034, 29781, 18470, 17496, 41158, 2006, 33512, 43956, 6236, 33166, 1678, 9014, 44020, 34818, 18470, 2006, 21263, 10027, 19052, 20772, 34818, 9390, 19052, 30200, 34818, 5972, 19052, 33653, 18470, 18470, 14168, 7575, 15467, 28312, 34818, 41158, 18470, 2006, 33512, 10027, 15467, 37723, 34818, 18154, 24240, 2006, 38978, 33166, 6863, 2006, 38978, 29781, 18470]\n",
      "test_create_instance.py [34558, 41572, 15467, 22480, 18017, 34123, 18017, 41572, 15467, 30166, 903, 6423, 34818, 34123, 18470, 2006, 21263, 7286, 19052, 1890, 9014, 28232, 34818, 8491, 18470, 2006, 33512, 6683, 19052, 8491, 15467, 7286, 6683, 30537, 32386, 6683, 30537, 7446, 6683, 30537, 40163, 6683, 30537, 33241, 15348, 19052, 18154, 31081, 2006, 18511, 33166, 23532, 2006, 12055, 33166, 16554, 2006, 29373, 24558, 15219, 33166, 3984, 2006, 4797, 29781, 8491, 15467, 34403, 34818, 6683, 33166, 15348, 18470, 2925, 1379, 1478, 22399, 2006, 21263, 23280, 15467, 4884, 34818, 18470]\n",
      "test_ajax.py [34558, 19930, 15467, 36361, 18017, 39886, 34558, 19930, 15467, 41692, 18017, 42736, 34558, 15467, 18143, 18017, 4431, 33166, 5756, 903, 38419, 34818, 42736, 18470, 2006, 21263, 9014, 589, 34818, 8491, 18470, 2006, 33512, 37221, 34818, 38419, 33166, 8491, 18470, 15467, 589, 34818, 18470, 8491, 15467, 32279, 19052, 4431, 34818, 8491, 15467, 24745, 18470, 8491, 15467, 32279, 15467, 1837, 34818, 18470, 9014, 7139, 34818, 8491, 18470, 2006, 33512, 8491, 15467, 32279, 15467, 40655, 34818, 18470, 8491, 15467, 32279, 15467, 10325, 34818, 18470, 8491, 15467, 23923, 34818, 8491, 15467, 32279, 15467, 12047, 33166, 38780, 18470, 9014, 26781, 34818, 8491, 18470, 2006, 33512, 8491, 15467, 32279, 15467, 24745, 15467, 35586, 34818, 25415, 18470, 14168, 8491, 15467, 15289, 34818, 39886, 18470, 28955, 8466, 2006, 28906, 8491, 15467, 32279, 15467, 10325, 34818, 17321, 19052, 13612, 18470, 8491, 15467, 30632, 34818, 2438, 33166, 8466, 15467, 33072, 15467, 43244, 34818, 18470, 18470, 903, 33416, 34818, 42736, 18470, 2006, 21263, 9014, 589, 34818, 8491, 18470, 2006, 33512, 37221, 34818, 33416, 33166, 8491, 18470, 15467, 589, 34818, 18470, 8491, 15467, 32279, 19052, 5756, 34818, 8491, 15467, 24745, 18470, 8491, 15467, 32279, 15467, 1837, 34818, 18470, 9014, 18051, 34818, 8491, 18470, 2006, 33512, 14168, 8491, 15467, 15289, 34818, 39886, 18470, 28955, 8466, 2006, 28906, 8491, 15467, 32279, 15467, 10325, 34818, 17321, 19052, 13612, 18470, 8491, 15467, 30632, 34818, 2438, 33166, 8466, 15467, 33072, 15467, 43244, 34818, 18470, 18470]\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(tokenized_data):\n",
    "    \"\"\"\n",
    "    Build a vocabulary mapping tokens to unique IDs.\n",
    "    \"\"\"\n",
    "    all_tokens = [token for tokens in tokenized_data.values() for token in tokens]\n",
    "    vocab = {token: idx for idx, token in enumerate(set(all_tokens), start=1)}  \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def convert_tokens_to_ids(tokenized_data, vocab):\n",
    "    \"\"\"\n",
    "    Convert tokenized data into token IDs using the vocabulary.\n",
    "    \"\"\"\n",
    "    token_ids_data = {\n",
    "        filename: [vocab[token] for token in tokens if token in vocab]\n",
    "        for filename, tokens in tokenized_data.items()\n",
    "    }\n",
    "    return token_ids_data\n",
    "\n",
    "vocab = build_vocabulary(tokenized_data)\n",
    "for idx, (keys, values) in enumerate(vocab.items()):\n",
    "    if idx >= 5:  \n",
    "        break\n",
    "    print(keys, values)\n",
    "\n",
    "token_ids_data = convert_tokens_to_ids(tokenized_data, vocab)\n",
    "for idx, (keys, values) in enumerate(token_ids_data.items()):\n",
    "    if idx >= 5:  \n",
    "        break\n",
    "    print(keys, values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"vocab_gru.pkl\", \"wb\") as file:\n",
    "    pickle.dump(vocab, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "Input: [34558, 34404, 18017, 13959], Output: 33166\n",
      "Input: [34404, 18017, 13959, 33166], Output: 25037\n",
      "Input: [18017, 13959, 33166, 25037], Output: 33166\n",
      "Input: [13959, 33166, 25037, 33166], Output: 7651\n",
      "Input: [33166, 25037, 33166, 7651], Output: 18017\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequences(token_ids_data, sequence_length=4):\n",
    "    \"\"\"\n",
    "    Prepare input-output pairs for training a model.\n",
    "    Each input is a sequence of token IDs, and the output is the next token ID.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    for token_ids in token_ids_data.values():\n",
    "        for i in range(len(token_ids) - sequence_length):\n",
    "            input_seq = token_ids[i:i + sequence_length]\n",
    "            output_token = token_ids[i + sequence_length]\n",
    "            x.append(input_seq)\n",
    "            y.append(output_token)\n",
    "    return x, y\n",
    "\n",
    "sequence_length = 4\n",
    "x,y = prepare_sequences(token_ids_data, sequence_length)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "for i in range(5):  \n",
    "    print(f\"Input: {x[i]}, Output: {y[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814010 814010\n"
     ]
    }
   ],
   "source": [
    "print(len(x), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651208 651208 162802 162802\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(y_train), len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 03:24:00.299688: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732917240.316134   85134 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732917240.321209   85134 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 03:24:00.339800: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/aniruddh/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "I0000 00:00:1732917241.837545   85134 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2791 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,GRU\n",
    "\n",
    "total_words = len(vocab) + 1\n",
    "max_sequence_len = 4\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
    "model.add(GRU(150,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(100))\n",
    "model.add(Dense(total_words,activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732917244.745343   85264 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 16ms/step - accuracy: 0.2639 - loss: 5.0615 - val_accuracy: 0.4603 - val_loss: 3.7063\n",
      "Epoch 2/10\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 16ms/step - accuracy: 0.4811 - loss: 3.4381 - val_accuracy: 0.5010 - val_loss: 3.4993\n",
      "Epoch 3/10\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 16ms/step - accuracy: 0.5299 - loss: 3.1169 - val_accuracy: 0.5186 - val_loss: 3.4042\n",
      "Epoch 4/10\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 16ms/step - accuracy: 0.5559 - loss: 2.9213 - val_accuracy: 0.5315 - val_loss: 3.3345\n",
      "Epoch 5/10\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 16ms/step - accuracy: 0.5722 - loss: 2.7977 - val_accuracy: 0.5386 - val_loss: 3.2919\n",
      "Epoch 6/10\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 16ms/step - accuracy: 0.5852 - loss: 2.6891 - val_accuracy: 0.5435 - val_loss: 3.2622\n",
      "Epoch 7/10\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 16ms/step - accuracy: 0.5949 - loss: 2.6141 - val_accuracy: 0.5454 - val_loss: 3.2372\n",
      "Epoch 8/10\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 16ms/step - accuracy: 0.6000 - loss: 2.5641 - val_accuracy: 0.5504 - val_loss: 3.2188\n",
      "Epoch 9/10\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 16ms/step - accuracy: 0.6056 - loss: 2.5156 - val_accuracy: 0.5504 - val_loss: 3.2074\n",
      "Epoch 10/10\n",
      "\u001b[1m20351/20351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 16ms/step - accuracy: 0.6101 - loss: 2.4869 - val_accuracy: 0.5521 - val_loss: 3.1985\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,415,700</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">113,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">75,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44157</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,459,857</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │     \u001b[38;5;34m4,415,700\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m150\u001b[0m)         │       \u001b[38;5;34m113,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m150\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m75,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44157\u001b[0m)          │     \u001b[38;5;34m4,459,857\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,193,673</span> (103.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,193,673\u001b[0m (103.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,064,557</span> (34.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,064,557\u001b[0m (34.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,129,116</span> (69.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m18,129,116\u001b[0m (69.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test),verbose=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word_custom(model, content):\n",
    "    tokens = tokenize.generate_tokens(iter(content.splitlines()).__next__)\n",
    "    token_list = []\n",
    "    for token in tokens:\n",
    "        if token.type not in (tokenize.COMMENT, tokenize.NL):\n",
    "            if token.string == '':\n",
    "                continue\n",
    "            token_list.append(token.string)\n",
    "    print(token_list)\n",
    "    count=0\n",
    "    print(len(vocab))\n",
    "    for i in range(len(token_list)):\n",
    "        if vocab.get(token_list[i])==None:\n",
    "            token_list[i] = 1\n",
    "        else:\n",
    "            token_list[i] = vocab[token_list[i]]\n",
    "    token_list = np.array(token_list).reshape(1, -1)\n",
    "    print(token_list)\n",
    "    prediction = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(prediction)\n",
    "    print(predicted_word_index)\n",
    "    flag = False\n",
    "    for key, value in vocab.items():\n",
    "        if value == predicted_word_index:\n",
    "            flag = True\n",
    "            return key\n",
    "    if(flag == False):\n",
    "        return \"No word found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:for i in\n",
      "['for', 'i', 'in']\n",
      "44156\n",
      "[[25812 31504 39957]]\n",
      "34818\n",
      "Next Word Prediction:(\n"
     ]
    }
   ],
   "source": [
    "input_text=\"for i in\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "next_word=predict_next_word_custom(model,input_text)\n",
    "print(f\"Next Word Prediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"GRU_big_25.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
